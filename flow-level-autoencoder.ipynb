{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement AutoEncoder based on frequency domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import Packet, Flow\n",
    "\n",
    "def get_flows(df: pd.DataFrame, key_type: str = \"default\") -> dict:\n",
    "    mp = dict()\n",
    "    for idx in range(len(df)): # simulate the process of packet processing\n",
    "        row = df.iloc[idx]\n",
    "        pkt = Packet(\n",
    "            src_ip=row[\"src_ip\"],\n",
    "            dst_ip=row[\"dst_ip\"],\n",
    "            src_port=row[\"src_port\"],\n",
    "            dst_port=row[\"dst_port\"],\n",
    "            protocol=row[\"protocol\"],\n",
    "            proto_code=row[\"proto_code\"],\n",
    "            pkt_length=row[\"pkt_length\"],\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            ttl=row[\"ttl\"],\n",
    "            tcp_window=row[\"tcp_window\"],\n",
    "            tcp_dataoffset=row[\"tcp_dataoffset\"],\n",
    "            udp_length=row[\"udp_length\"],\n",
    "            label=row[\"label\"],\n",
    "        )\n",
    "        key = pkt.key(type=key_type)\n",
    "        if key not in mp:\n",
    "            mp[key] = Flow()\n",
    "        mp[key].add_packet(pkt)\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import whisper_config\n",
    "\n",
    "def transform(mp: dict, feature_type: str = \"whisper\", \n",
    "              data_type: str = \"train\", test_data_aug: bool = True):\n",
    "    packet_data, flow_data = [], []\n",
    "    packet_labels, flow_labels = [], []\n",
    "    for key, flow in mp.items():\n",
    "        vec = flow.vector(feature_type=feature_type)\n",
    "        if feature_type == \"whisper\":\n",
    "            if len(vec) <= (whisper_config[\"n_fft\"] // 2):\n",
    "                # packet level features\n",
    "                # vec = flow.packet_vector(agg_type=\"mean\") + flow.packet_vector(agg_type=\"std\") \\\n",
    "                #     + flow.packet_vector(agg_type=\"max\") + flow.packet_vector(agg_type=\"min\")\n",
    "                # packet_data.append(vec)\n",
    "                # packet_labels.append(flow.label)\n",
    "\n",
    "                # implement fft on short flows\n",
    "                ten = torch.tensor(vec)\n",
    "                ten_fft = torch.fft.fft(ten, n=(whisper_config[\"n_fft\"] // 2)+1)\n",
    "                ten_power = torch.pow(ten_fft.real, 2) + torch.pow(ten_fft.imag, 2)\n",
    "                ten_res = (ten_power.squeeze()+1).log2()\n",
    "                ten_res = torch.where(torch.isnan(ten_res), torch.zeros_like(ten_res), ten_res)\n",
    "                ten_res = torch.where(torch.isinf(ten_res), torch.zeros_like(ten_res), ten_res)\n",
    "                if data_type == \"test\" and test_data_aug:\n",
    "                    # data shape for test data augmentation: (n_flow, n_sample, floor(n_fft/2)+1)\n",
    "                    packet_data.append([ten_res.tolist()])\n",
    "                else:\n",
    "                    # data shape for no data augmentation: (n_flow, floor(n_fft/2)+1)\n",
    "                    packet_data.append(ten_res.tolist())\n",
    "                packet_labels.append(flow.label)\n",
    "                \n",
    "            else:\n",
    "                # flow level featrues\n",
    "                ten = torch.tensor(vec)\n",
    "                # stft requirement: input_size > (n_fft // 2)\n",
    "                # default return shape: (floor(n_fft/2)+1, n_frame, 2)\n",
    "                ten_fft = torch.stft(ten, whisper_config[\"n_fft\"])\n",
    "                ten_power = torch.pow(ten_fft[:,:,0], 2) + torch.pow(ten_fft[:,:,1], 2)\n",
    "                ten_res = ((ten_power.squeeze()+1).log2()).permute(1,0)\n",
    "                ten_res = torch.where(torch.isnan(ten_res), torch.zeros_like(ten_res), ten_res)\n",
    "                ten_res = torch.where(torch.isinf(ten_res), torch.zeros_like(ten_res), ten_res)\n",
    "                # ten_res shape: (n_frame, floor(n_fft/2)+1)\n",
    "                if data_type == \"train\":\n",
    "                    if (ten_res.size(0) > whisper_config[\"mean_win_train\"]):\n",
    "                        for _ in range(whisper_config[\"num_train_sample\"]):\n",
    "                            start_idx = torch.randint(0, ten_res.size(0)\n",
    "                                        - whisper_config[\"mean_win_train\"], (1,)).item()\n",
    "                            ten_tmp = ten_res[start_idx:start_idx+whisper_config[\"mean_win_train\"],:].mean(dim=0)\n",
    "                            flow_data.append(ten_tmp.tolist())\n",
    "                            flow_labels.append(flow.label)\n",
    "                    else:\n",
    "                        flow_data.append(ten_res.mean(dim=0).tolist())\n",
    "                        flow_labels.append(flow.label)\n",
    "                else: # for test\n",
    "                    if test_data_aug:\n",
    "                        tmp_data = []\n",
    "                        if (ten_res.size(0) > whisper_config[\"mean_win_test\"]):\n",
    "                            # data augmentation for kmeans on flows with length > mean_win_test\n",
    "                            for idx in range(0, ten_res.size(0) - whisper_config[\"mean_win_test\"], \n",
    "                                            whisper_config[\"mean_win_test\"]):\n",
    "                                ten_tmp = ten_res[idx:idx+whisper_config[\"mean_win_test\"],:].mean(dim=0)\n",
    "                                tmp_data.append(ten_tmp.tolist())\n",
    "                        else:\n",
    "                            # no data augmentation for kmeans on flows with length < mean_win_test\n",
    "                            tmp_data.append(ten_res.mean(dim=0).tolist())\n",
    "                        flow_data.append(tmp_data)\n",
    "                        # data shape for augmentation: (n_flow, n_sample, floor(n_fft/2)+1)\n",
    "                    else: # for other detection methods\n",
    "                        flow_data.append(ten_res.mean(dim=0).tolist())\n",
    "                        # data shape for no augmentation: (n_flow, floor(n_fft/2)+1)\n",
    "                    flow_labels.append(flow.label)\n",
    "        elif feature_type == \"encoding\":\n",
    "            # directly use the whisper encoding vector\n",
    "            pass\n",
    "        else: # for other feature types\n",
    "            pass\n",
    "    return packet_data, packet_labels, flow_data, flow_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "USE_SHORT_FLOW = True\n",
    "\n",
    "train_benign_filename = \"dataset/benign_small.csv\"\n",
    "train_df = pd.read_csv(train_benign_filename)\n",
    "train_df[\"label\"] = 1\n",
    "train_packet_data, train_packet_labels, train_flow_data, train_flow_labels = transform(get_flows(train_df))\n",
    "train_data = train_flow_data if not USE_SHORT_FLOW else train_flow_data + train_packet_data\n",
    "train_labels = train_flow_labels if not USE_SHORT_FLOW else train_flow_labels + train_packet_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def train_ae(train_data, train_labels, save_dir,\n",
    "            model, criterion, optimizer, device, \n",
    "            batch_size=32, num_epochs=200):\n",
    "    train_dataset = Dataset(train_data, train_labels)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, labels in train_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    model_save_path = os.path.join(save_dir, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    loss_save_path = os.path.join(save_dir, \"train_loss.json\")\n",
    "    with open(loss_save_path, \"w\") as f:\n",
    "        json.dump(loss_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 326.4511\n",
      "Epoch 2/200, Loss: 331.1851\n",
      "Epoch 3/200, Loss: 326.5228\n",
      "Epoch 4/200, Loss: 322.1415\n",
      "Epoch 5/200, Loss: 347.4643\n",
      "Epoch 6/200, Loss: 342.5143\n",
      "Epoch 7/200, Loss: 326.0724\n",
      "Epoch 8/200, Loss: 360.1368\n",
      "Epoch 9/200, Loss: 319.1062\n",
      "Epoch 10/200, Loss: 321.9159\n",
      "Epoch 11/200, Loss: 342.9333\n",
      "Epoch 12/200, Loss: 339.3370\n",
      "Epoch 13/200, Loss: 320.7643\n",
      "Epoch 14/200, Loss: 333.5533\n",
      "Epoch 15/200, Loss: 362.2047\n",
      "Epoch 16/200, Loss: 353.7042\n",
      "Epoch 17/200, Loss: 341.9458\n",
      "Epoch 18/200, Loss: 332.1481\n",
      "Epoch 19/200, Loss: 332.5726\n",
      "Epoch 20/200, Loss: 346.9189\n",
      "Epoch 21/200, Loss: 339.5843\n",
      "Epoch 22/200, Loss: 343.4209\n",
      "Epoch 23/200, Loss: 323.4227\n",
      "Epoch 24/200, Loss: 340.4665\n",
      "Epoch 25/200, Loss: 321.7179\n",
      "Epoch 26/200, Loss: 346.6592\n",
      "Epoch 27/200, Loss: 326.8077\n",
      "Epoch 28/200, Loss: 341.7108\n",
      "Epoch 29/200, Loss: 344.3173\n",
      "Epoch 30/200, Loss: 356.8511\n",
      "Epoch 31/200, Loss: 337.2407\n",
      "Epoch 32/200, Loss: 340.0748\n",
      "Epoch 33/200, Loss: 329.4157\n",
      "Epoch 34/200, Loss: 336.0526\n",
      "Epoch 35/200, Loss: 338.8449\n",
      "Epoch 36/200, Loss: 352.5050\n",
      "Epoch 37/200, Loss: 321.0223\n",
      "Epoch 38/200, Loss: 325.2512\n",
      "Epoch 39/200, Loss: 322.4238\n",
      "Epoch 40/200, Loss: 341.5385\n",
      "Epoch 41/200, Loss: 334.2906\n",
      "Epoch 42/200, Loss: 351.6931\n",
      "Epoch 43/200, Loss: 343.9807\n",
      "Epoch 44/200, Loss: 329.4348\n",
      "Epoch 45/200, Loss: 341.0197\n",
      "Epoch 46/200, Loss: 327.1371\n",
      "Epoch 47/200, Loss: 341.9844\n",
      "Epoch 48/200, Loss: 333.9015\n",
      "Epoch 49/200, Loss: 327.9827\n",
      "Epoch 50/200, Loss: 352.0653\n",
      "Epoch 51/200, Loss: 358.1807\n",
      "Epoch 52/200, Loss: 311.5956\n",
      "Epoch 53/200, Loss: 356.9120\n",
      "Epoch 54/200, Loss: 330.0716\n",
      "Epoch 55/200, Loss: 339.9882\n",
      "Epoch 56/200, Loss: 335.2274\n",
      "Epoch 57/200, Loss: 325.2906\n",
      "Epoch 58/200, Loss: 330.3704\n",
      "Epoch 59/200, Loss: 325.8687\n",
      "Epoch 60/200, Loss: 349.7936\n",
      "Epoch 61/200, Loss: 333.3262\n",
      "Epoch 62/200, Loss: 338.9619\n",
      "Epoch 63/200, Loss: 347.0580\n",
      "Epoch 64/200, Loss: 337.9060\n",
      "Epoch 65/200, Loss: 343.7922\n",
      "Epoch 66/200, Loss: 324.1594\n",
      "Epoch 67/200, Loss: 337.4877\n"
     ]
    }
   ],
   "source": [
    "from config import whisper_config\n",
    "import torch\n",
    "import os\n",
    "\n",
    "input_dim = whisper_config[\"n_fft\"] // 2 + 1\n",
    "hidden_dim = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoEncoder(input_dim, hidden_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "save_dir = os.path.join(\"model\", \"whisper\", \"autoencoder\", os.path.basename(train_benign_filename))\n",
    "train_ae(torch.tensor(train_data), torch.tensor(train_labels),\n",
    "        save_dir, model, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "benign_filenames = [os.path.join(\"train_set\", \"benign\" + str(i) + \".csv\") \n",
    "                    for i in range(1, 3)] + [\"dataset_lite/mirai-benign.csv\"]\n",
    "attack_filenames = [os.path.join(\"dataset_lite\", x+\".csv\") for x in \n",
    "                    [\"BruteForce-Web\", \"BruteForce-XSS\", \"infiltration\", \n",
    "                    \"osscan\", \"SQL_Injection\", \"ssldosA10only\", \"mirai-attack\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
