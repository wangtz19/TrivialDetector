{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract flow level features by FlowLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import Packet, Flow\n",
    "\n",
    "def get_flows(df: pd.DataFrame, key_type: str = \"default\") -> dict:\n",
    "    mp = dict()\n",
    "    for idx in range(len(df)): # simulate the process of packet processing\n",
    "        row = df.iloc[idx]\n",
    "        pkt = Packet(\n",
    "            src_ip=row[\"src_ip\"],\n",
    "            dst_ip=row[\"dst_ip\"],\n",
    "            src_port=row[\"src_port\"],\n",
    "            dst_port=row[\"dst_port\"],\n",
    "            protocol=row[\"protocol\"],\n",
    "            proto_code=row[\"proto_code\"],\n",
    "            pkt_length=row[\"pkt_length\"],\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            ttl=row[\"ttl\"],\n",
    "            tcp_window=row[\"tcp_window\"],\n",
    "            tcp_dataoffset=row[\"tcp_dataoffset\"],\n",
    "            udp_length=row[\"udp_length\"],\n",
    "            label=row[\"label\"],\n",
    "        )\n",
    "        key = pkt.key(type=key_type)\n",
    "        if key not in mp:\n",
    "            mp[key] = Flow()\n",
    "        mp[key].add_packet(pkt)\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform(mp: dict, feature_type: str = \"whisper\"):\n",
    "    flow_data, flow_labels = [], []\n",
    "    for key, flow in mp.items():\n",
    "        if feature_type == \"whisper\":\n",
    "            pass\n",
    "        elif feature_type == \"flowlens\":\n",
    "            vec_size, vec_time, label = flow.vector(feature_type=feature_type)\n",
    "            flow_data.append(vec_size+vec_time)\n",
    "            flow_labels.append(label)\n",
    "    return flow_data, flow_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train & test with supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import json\n",
    "import skops.io as sio\n",
    "\n",
    "def train(data, labels, save_path):\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    clf.fit(data, labels)\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    sio.dump(clf, save_path)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(data, labels, load_path):\n",
    "    clf = sio.load(load_path, True)\n",
    "    pred = clf.predict(data)\n",
    "    return accuracy_score(labels, pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix benign and attack traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_train_test_df(benign_path: str, attack_path: str):\n",
    "    df_benign = pd.read_csv(benign_path)\n",
    "    df_benign[\"label\"] = 0\n",
    "    df_attack = pd.read_csv(attack_path)\n",
    "    df_attack[\"label\"] = 1\n",
    "    df_mix = pd.concat([df_benign, df_attack], ignore_index=True, axis=0)\n",
    "    df_group = df_mix.groupby([\"src_ip\", \"dst_ip\", \"src_port\", \"dst_port\", \"protocol\"])\n",
    "    df_mix[\"group\"] = df_group.ngroup()\n",
    "    df_mix[\"group\"] = df_mix[\"group\"] % 10\n",
    "    df_mix[\"group\"] = df_mix[\"group\"].apply(lambda x: \"train\" if x < 6 else \"test\")\n",
    "    # df_mix[\"group\"].apply(lambda x: \"train\" if np.random.rand() < train_test_ratio else \"test\")\n",
    "    df_train = df_mix[df_mix[\"group\"] == \"train\"]\n",
    "    df_test = df_mix[df_mix[\"group\"] == \"test\"]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "benign_filenames = [os.path.join(\"train_set\", \"benign\" + str(i) + \".csv\") for i in range(1, 6)]\n",
    "attack_filenames = [os.path.join(\"dataset_lite\", x) for x in os.listdir(\"dataset_lite\") if x.endswith(\".csv\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new model for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train df size: 117179, test df size: 76630\n",
      "accuracy of dataset_lite/osscan.csv: 0.9910931174089069\n",
      "train df size: 178411, test df size: 63349\n",
      "accuracy of dataset_lite/ssldosA10only.csv: 0.9993089149965446\n",
      "train df size: 122020, test df size: 73210\n",
      "accuracy of dataset_lite/infiltration.csv: 1.0\n",
      "train df size: 133066, test df size: 76658\n",
      "accuracy of dataset_lite/BruteForce-Web.csv: 1.0\n",
      "train df size: 122227, test df size: 69423\n",
      "accuracy of dataset_lite/SQL_Injection.csv: 0.9996550534667127\n",
      "train df size: 234029, test df size: 155144\n",
      "accuracy of dataset_lite/mirai.csv: 0.9962952408093474\n",
      "train df size: 123785, test df size: 79386\n",
      "accuracy of dataset_lite/BruteForce-XSS.csv: 0.999657651489216\n"
     ]
    }
   ],
   "source": [
    "acc_dict = dict()\n",
    "all_flows = True\n",
    "\n",
    "benign_filename = benign_filenames[0]\n",
    "for attack_filename in attack_filenames:\n",
    "    df_train, df_test = get_train_test_df(benign_filename, attack_filename)\n",
    "    print(f\"train df size: {len(df_train)}, test df size: {len(df_test)}\")\n",
    "    train_data, train_labels = transform(get_flows(df_train), feature_type=\"flowlens\")\n",
    "    if train_data is None:\n",
    "        continue\n",
    "    save_path = os.path.join(\"model\", \"flowlens\", attack_filename + \".skops\")\n",
    "    train(train_data, train_labels, save_path)\n",
    "    test_data, test_labels = transform(get_flows(df_test), feature_type=\"flowlens\")\n",
    "    acc = test(test_data, test_labels, save_path)\n",
    "    print(f\"accuracy of {attack_filename}: {acc}\")\n",
    "    acc_dict[attack_filename] = acc\n",
    "\n",
    "import json, os\n",
    "suffix = \"all_flows\" if all_flows else \"short_flows\"\n",
    "save_path = os.path.join(\"result\", \"flowlens\", f\"{suffix}.json\")\n",
    "if not os.path.exists(os.path.dirname(save_path)):\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "with open(os.path.join(save_path), \"w\") as f:\n",
    "    json.dump(acc_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train only one model for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_df(benign_path: str = None, attack_path: str = None):\n",
    "    assert benign_path is not None or attack_path is not None, \\\n",
    "        \"benign_path and attack_path cannot be None at the same time\"\n",
    "    if benign_path is not None:\n",
    "        df_benign = pd.read_csv(benign_path)\n",
    "        df_benign[\"label\"] = 0\n",
    "    else:\n",
    "        df_benign = None\n",
    "    if attack_path is not None:\n",
    "        df_attack = pd.read_csv(attack_path)\n",
    "        df_attack[\"label\"] = 1\n",
    "    else:\n",
    "        df_attack = None\n",
    "    df_mix = pd.concat([df_benign, df_attack], ignore_index=True, axis=0)\n",
    "    return df_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# train_benign_filename = benign_filenames[0]\n",
    "# train_attack_filename = attack_filenames[0]\n",
    "train_benign_filename = \"dataset_lite/mirai-benign.csv\"\n",
    "train_attack_filename = \"dataset_lite/mirai-attack.csv\"\n",
    "\n",
    "df_train = get_mix_df(benign_path=train_benign_filename, attack_path=train_attack_filename)\n",
    "train_data, train_labels = transform(get_flows(df_train), feature_type=\"flowlens\")\n",
    "save_path = os.path.join(\"model\", \"flowlens\", \n",
    "            os.path.basename(train_benign_filename) + \"_\" + \\\n",
    "            os.path.basename(train_attack_filename) + \".skops\")\n",
    "train(train_data, train_labels, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of train_set/benign2.csv: 0.054839968774395\n",
      "accuracy of train_set/benign3.csv: 0.04937845303867403\n",
      "accuracy of train_set/benign4.csv: 0.05924520345772718\n",
      "accuracy of train_set/benign5.csv: 0.057473684210526316\n",
      "accuracy of dataset_lite/osscan.csv: 0.9912023460410557\n",
      "accuracy of dataset_lite/ssldosA10only.csv: 0.6956521739130435\n",
      "accuracy of dataset_lite/infiltration.csv: 0.6666666666666666\n",
      "accuracy of dataset_lite/BruteForce-Web.csv: 1.0\n",
      "accuracy of dataset_lite/SQL_Injection.csv: 1.0\n",
      "accuracy of dataset_lite/mirai.csv: 0.9354547937721934\n",
      "accuracy of dataset_lite/mirai-benign.csv: 0.9916708915767365\n",
      "accuracy of dataset_lite/BruteForce-XSS.csv: 1.0\n"
     ]
    }
   ],
   "source": [
    "for test_benign_filename in benign_filenames[1:]:\n",
    "    df_test = get_mix_df(benign_path=test_benign_filename)\n",
    "    test_data, test_labels = transform(get_flows(df_test), feature_type=\"flowlens\")\n",
    "    acc = test(test_data, test_labels, save_path)\n",
    "    print(f\"accuracy of {test_benign_filename}: {acc}\")\n",
    "\n",
    "for test_attack_filename in attack_filenames[1:]:\n",
    "    df_test = get_mix_df(attack_path=test_attack_filename)\n",
    "    test_data, test_labels = transform(get_flows(df_test), feature_type=\"flowlens\")\n",
    "    acc = test(test_data, test_labels, save_path)\n",
    "    print(f\"accuracy of {test_attack_filename}: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train & test with unsupervised learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train in a zero-positive way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/PRO/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def train(train_data, save_path, n_clusters):\n",
    "    train_data = torch.tensor(train_data).float()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(train_data.cpu().numpy())\n",
    "\n",
    "    centroids = torch.tensor(kmeans.cluster_centers_)\n",
    "    train_loss = torch.cdist(train_data, centroids, p=2).min(dim=1).values.mean()\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"centroids\": centroids.tolist(),\n",
    "            \"train_loss\": train_loss.item(),\n",
    "        }, f)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def test(test_data, test_labels, load_path, scale=5):\n",
    "    with open(load_path, \"r\") as f:\n",
    "        model_param = json.load(f)\n",
    "    centroids = torch.tensor(model_param[\"centroids\"])\n",
    "    train_loss = model_param[\"train_loss\"]\n",
    "\n",
    "    pred = []\n",
    "    for vec in test_data:\n",
    "        vec = torch.tensor(vec).float()\n",
    "        dist = torch.cdist(vec.unsqueeze(0), centroids, p=2).min(dim=1).values\n",
    "        pred.append(1 if dist > scale * train_loss else 0)\n",
    "    return accuracy_score(test_labels, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/PRO/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# train_benign_filename = benign_filenames[0]\n",
    "train_benign_filename = os.path.join(\"dataset_lite\", \"mirai-benign.csv\")\n",
    "train_df = pd.read_csv(train_benign_filename)\n",
    "train_df[\"label\"] = \"unknown\"\n",
    "train_data, _ = transform(get_flows(train_df), feature_type=\"flowlens\")\n",
    "\n",
    "from config import whisper_config\n",
    "save_path = os.path.join(\"model\", \"flowlens\", \"kmeans.json\")\n",
    "train(train_data, save_path, whisper_config[\"val_K\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of train_set/benign2.csv: 0.7950819672131147\n",
      "accuracy of train_set/benign3.csv: 0.805939226519337\n",
      "accuracy of train_set/benign4.csv: 0.7853679106051022\n",
      "accuracy of train_set/benign5.csv: 0.7991578947368421\n",
      "accuracy of dataset_lite/mirai-attack.csv: 0.005329379708286584\n",
      "accuracy of dataset_lite/osscan.csv: 0.005376344086021506\n",
      "accuracy of dataset_lite/ssldosA10only.csv: 0.043478260869565216\n",
      "accuracy of dataset_lite/infiltration.csv: 1.0\n",
      "accuracy of dataset_lite/BruteForce-Web.csv: 0.5036496350364964\n",
      "accuracy of dataset_lite/SQL_Injection.csv: 0.0\n",
      "accuracy of dataset_lite/mirai.csv: 0.004561595192570336\n",
      "accuracy of dataset_lite/mirai-benign.csv: 0.002462519012095314\n",
      "accuracy of dataset_lite/BruteForce-XSS.csv: 0.9302325581395349\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result/flowlens/mirai-benign.csv/all-accuracy.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m accuracy_base_name \u001b[39m=\u001b[39m  \u001b[39m\"\u001b[39m\u001b[39mall-accuracy.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m accuracy_save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mflowlens\u001b[39m\u001b[39m\"\u001b[39m, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(train_benign_filename), accuracy_base_name)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(accuracy_save_path, \u001b[39m\"\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m     json\u001b[39m.\u001b[39mdump(accuracy_dict, f)\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result/flowlens/mirai-benign.csv/all-accuracy.json'"
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "\n",
    "for test_benign_filename in benign_filenames[1:]:\n",
    "    test_df = pd.read_csv(test_benign_filename)\n",
    "    test_df[\"label\"] = 0\n",
    "    test_data, test_lables = transform(get_flows(test_df), feature_type=\"flowlens\")\n",
    "    acc = test(test_data, test_lables, save_path)\n",
    "    print(f\"accuracy of {test_benign_filename}: {acc}\")\n",
    "    accuracy_dict[test_benign_filename] = acc\n",
    "\n",
    "for test_attack_filename in attack_filenames:\n",
    "    test_df = pd.read_csv(test_attack_filename)\n",
    "    test_df[\"label\"] = 1\n",
    "    test_data, test_lables = transform(get_flows(test_df), feature_type=\"flowlens\")\n",
    "    acc = test(test_data, test_lables, save_path)\n",
    "    print(f\"accuracy of {test_attack_filename}: {acc}\")\n",
    "    accuracy_dict[test_attack_filename] = acc\n",
    "\n",
    "accuracy_base_name =  \"all-accuracy.json\"\n",
    "accuracy_save_path = os.path.join(\"result\", \"flowlens\", os.path.basename(train_benign_filename), accuracy_base_name)\n",
    "os.makedirs(os.path.dirname(accuracy_save_path), exist_ok=True)\n",
    "with open(accuracy_save_path, \"w\") as f:\n",
    "    json.dump(accuracy_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
