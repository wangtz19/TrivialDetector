{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import Packet, Flow\n",
    "\n",
    "def get_flows(df: pd.DataFrame, key_type: str = \"default\") -> dict:\n",
    "    mp = dict()\n",
    "    for idx in range(len(df)): # simulate the process of packet processing\n",
    "        row = df.iloc[idx]\n",
    "        pkt = Packet(\n",
    "            src_addr=row[\"src_addr\"],\n",
    "            dst_addr=row[\"dst_addr\"],\n",
    "            src_ip=row[\"src_ip\"],\n",
    "            dst_ip=row[\"dst_ip\"],\n",
    "            src_port=row[\"src_port\"],\n",
    "            dst_port=row[\"dst_port\"],\n",
    "            protocol=row[\"protocol\"],\n",
    "            proto_code=row[\"proto_code\"],\n",
    "            pkt_length=row[\"pkt_length\"],\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            ttl=row[\"ttl\"],\n",
    "            tcp_window=row[\"tcp_window\"],\n",
    "            tcp_dataoffset=row[\"tcp_dataoffset\"],\n",
    "            udp_length=row[\"udp_length\"],\n",
    "        )\n",
    "        key = pkt.key(type=key_type)\n",
    "        if key not in mp:\n",
    "            mp[key] = Flow()\n",
    "        mp[key].add_packet(pkt)\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/PRO/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import whisper_config\n",
    "\n",
    "def transform(mp: dict, feature_type: str = \"whisper\", data_type: str = \"train\"):\n",
    "    train_packet_data, train_flow_data = [], []\n",
    "    test_packet_data, test_flow_data = {}, {}\n",
    "    for key, flow in mp.items():\n",
    "        vec = flow.vector(feature_type=feature_type)\n",
    "        if feature_type == \"whisper\":\n",
    "            if len(vec) <= (whisper_config[\"n_fft\"] // 2):\n",
    "                # packet level features\n",
    "                vec = flow.packet_vector(agg_type=\"mean\") + flow.packet_vector(agg_type=\"std\") \\\n",
    "                    + flow.packet_vector(agg_type=\"max\") + flow.packet_vector(agg_type=\"min\")\n",
    "                if data_type == \"train\":\n",
    "                    train_packet_data.append(vec)\n",
    "                else: # for test\n",
    "                    test_packet_data[key] = vec\n",
    "            else:\n",
    "                # flow level featrues\n",
    "                ten = torch.tensor(vec)\n",
    "                # stft requirement: input_size > (n_fft // 2)\n",
    "                # default return shape: (floor(n_fft/2)+1, n_frame, 2)\n",
    "                ten_fft = torch.stft(ten, whisper_config[\"n_fft\"])\n",
    "                ten_power = torch.pow(ten_fft[:,:,0], 2) + torch.pow(ten_fft[:,:,1], 2)\n",
    "                ten_res = ((ten_power.squeeze()+1).log2()).permute(1,0)\n",
    "                ten_res = torch.where(torch.isnan(ten_res), torch.zeros_like(ten_res), ten_res)\n",
    "                ten_res = torch.where(torch.isinf(ten_res), torch.zeros_like(ten_res), ten_res)\n",
    "                if data_type == \"train\":\n",
    "                    if (ten_res.size(0) > whisper_config[\"mean_win_train\"]):\n",
    "                        for _ in range(whisper_config[\"num_train_sample\"]):\n",
    "                            start_idx = torch.randint(0, ten_res.size(0)\n",
    "                                        - whisper_config[\"mean_win_train\"], (1,)).item()\n",
    "                            ten_tmp = ten_res[start_idx:start_idx+whisper_config[\"mean_win_train\"],:].mean(dim=0)\n",
    "                            train_flow_data.append(ten_tmp.tolist())\n",
    "                    else:\n",
    "                        train_flow_data.append(ten_res.mean(dim=0).tolist())\n",
    "                else: # for test\n",
    "                    tmp_data = []\n",
    "                    if (ten_res.size(0) > whisper_config[\"mean_win_test\"]):\n",
    "                        for idx in range(0, ten_res.size(0) - whisper_config[\"mean_win_test\"], \n",
    "                                        whisper_config[\"mean_win_test\"]):\n",
    "                            ten_tmp = ten_res[idx:idx+whisper_config[\"mean_win_test\"],:].mean(dim=0)\n",
    "                            tmp_data.append(ten_tmp.tolist())\n",
    "                    else:\n",
    "                        tmp_data.append(ten_res.mean(dim=0).tolist())\n",
    "                    test_flow_data[key] = tmp_data\n",
    "        else: # for other feature types\n",
    "            pass\n",
    "    if data_type == \"train\":\n",
    "        return train_packet_data, train_flow_data\n",
    "    else:\n",
    "        return test_packet_data, test_flow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7321, 8.7178],\n",
      "        [4.8990, 4.3589],\n",
      "        [9.9499, 4.0000],\n",
      "        [2.4495, 7.2801]])\n",
      "tensor([1.7321, 4.3589, 4.0000, 2.4495])\n",
      "tensor(3.1351)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [3,2,6]]).float()\n",
    "c = torch.tensor([[2,1,4], [7,8,5]]).float()\n",
    "dist = torch.cdist(a, c, p=2)\n",
    "print(dist)\n",
    "print(dist.min(dim=1).values)\n",
    "print(dist.min(dim=1).values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train(train_data, save_path, n_clusters):\n",
    "    train_data = torch.tensor(train_data)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(train_data.cpu().numpy())\n",
    "\n",
    "    centroids = torch.tensor(kmeans.cluster_centers_)\n",
    "    train_loss = torch.cdist(train_data, centroids, p=2).min(dim=1).values.mean()\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"centroids\": centroids.tolist(),\n",
    "            \"train_loss\": train_loss.item(),\n",
    "        }, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train_file = os.path.join(\"train_set\", \"benign1.csv\")\n",
    "train_df = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train_flow_dict = get_flows(train_df, key_type=\"whisper\") # merge flow by source ip\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_flow_dict \u001b[39m=\u001b[39m get_flows(train_df) \u001b[39m# merge flow by 5-tuple\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m, in \u001b[0;36mget_flows\u001b[0;34m(df, key_type)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)): \u001b[39m# simulate the process of packet processing\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     row \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[idx]\n\u001b[1;32m      8\u001b[0m     pkt \u001b[39m=\u001b[39m Packet(\n\u001b[1;32m      9\u001b[0m         src_addr\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39msrc_addr\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m         dst_addr\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mdst_addr\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m         src_ip\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39msrc_ip\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m         dst_ip\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mdst_ip\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m         src_port\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39msrc_port\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m---> 14\u001b[0m         dst_port\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39;49m\u001b[39mdst_port\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     15\u001b[0m         protocol\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mprotocol\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m         proto_code\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mproto_code\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m         pkt_length\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mpkt_length\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m         timestamp\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m         ttl\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mttl\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m         tcp_window\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mtcp_window\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m         tcp_dataoffset\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mtcp_dataoffset\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     22\u001b[0m         udp_length\u001b[39m=\u001b[39mrow[\u001b[39m\"\u001b[39m\u001b[39mudp_length\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     key \u001b[39m=\u001b[39m pkt\u001b[39m.\u001b[39mkey(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mkey_type)\n\u001b[1;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mp:\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/core/series.py:1090\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_loc(label)\n\u001b[0;32m-> 1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49m_get_values_for_loc(\u001b[39mself\u001b[39;49m, loc, label)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_flow_dict = get_flows(train_df, key_type=\"whisper\") # merge flow by source ip\n",
    "train_flow_dict = get_flows(train_df) # merge flow by 5-tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/PRO/lib/python3.10/site-packages/torch/functional.py:632: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# train_data = transform(train_flow_dict, feature_type=\"whisper\", data_type=\"train\")\n",
    "train_packet_data, train_flow_data = transform(train_flow_dict, feature_type=\"whisper\", data_type=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/PRO/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRO/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join(\"model\", \"whisper\", \"train_packet.json\")\n",
    "train(train_packet_data, save_path, n_clusters=whisper_config[\"val_K\"])\n",
    "\n",
    "save_path = os.path.join(\"model\", \"whisper\", \"train_flow.json\")\n",
    "train(train_flow_data, save_path, n_clusters=whisper_config[\"val_K\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def test(test_data, load_path, save_path):\n",
    "    with open(load_path, \"r\") as f:\n",
    "        centroids = json.load(f)[\"centroids\"]\n",
    "    centroids = torch.tensor(centroids)\n",
    "    \n",
    "    test_res = []\n",
    "    for key, val in test_data.items():\n",
    "        val = torch.tensor(val)\n",
    "        if (val.size(0) > whisper_config[\"mean_win_test\"]):\n",
    "            max_dist = 0\n",
    "            for idx in range(0, val.size(0) - whisper_config[\"mean_win_test\"], \n",
    "                             whisper_config[\"mean_win_test\"]):\n",
    "                ten_tmp = val[idx:idx+whisper_config[\"mean_win_test\"],:].mean(dim=0)\n",
    "                dist = torch.norm(ten_tmp - centroids, dim=1).min()\n",
    "                max_dist = max(max_dist, dist)\n",
    "            min_dist = max_dist\n",
    "        else:\n",
    "            min_dist = torch.norm(val.mean(dim=0) - centroids, dim=1).min()\n",
    "        test_res.append({\"key\": key, \"loss\": min_dist.item()})\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(test_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_set/benign_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# test_file = os.path.join(\"dataset_lite\", \"ssldosA10only.csv\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mtrain_set\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbenign_test.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(test_file)\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/PRO/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_set/benign_test.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# test_file = os.path.join(\"dataset_lite\", \"ssldosA10only.csv\")\n",
    "test_file = os.path.join(\"train_set\", \"benign2.csv\")\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_flow_dict = get_flows(test_df, key_type=\"whisper\")\n",
    "test_flow_dict = get_flows(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_packet_data, test_flow_data = transform(test_flow_dict, feature_type=\"whisper\", data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = os.path.join(\"model\", \"whisper\", \"train_packet.json\")\n",
    "save_path = os.path.join(\"result\", \"whisper\", test_file.split(os.sep)[-1].split(\".\")[0] + \"-packet.json\")\n",
    "test(test_packet_data, load_path, save_path)\n",
    "\n",
    "load_path = os.path.join(\"model\", \"whisper\", \"train_flow.json\")\n",
    "save_path = os.path.join(\"result\", \"whisper\", test_file.split(os.sep)[-1].split(\".\")[0] + \"-flow.json\")\n",
    "test(test_flow_data, load_path, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix benign and attack traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     src_addr    dst_addr          src_ip          dst_ip  src_port  dst_port  \\\n",
      "0  1902274041  2516971618  113.98.101.249    150.5.240.98   51631.0      80.0   \n",
      "1  2648454693  3699660524   157.220.54.37  220.132.86.236    4500.0    4500.0   \n",
      "2  1902274041  2516971618  113.98.101.249    150.5.240.98   51631.0      80.0   \n",
      "3  1902274041  2516971618  113.98.101.249    150.5.240.98   51631.0      80.0   \n",
      "4  1902274041  2516971618  113.98.101.249    150.5.240.98   51631.0      80.0   \n",
      "\n",
      "   protocol  proto_code  pkt_length     timestamp  tos     id  ttl  chksum  \\\n",
      "0         6        1000          54  1.591765e+09    0  56155  122   51120   \n",
      "1        17           3          42  1.591765e+09    0  23255   56    6996   \n",
      "2         6        1000          54  1.591765e+09    0  56155  122   51120   \n",
      "3         6        1000          54  1.591765e+09    0  56156  122   51119   \n",
      "4         6        1000          54  1.591765e+09    0  56156  122   51119   \n",
      "\n",
      "   flags  tcp_window  tcp_dataoffset  udp_length payload   label  \n",
      "0  16384      8235.0             5.0         NaN     b''  BENIGN  \n",
      "1      0         NaN             NaN      1340.0     b''  BENIGN  \n",
      "2  16384      8235.0             5.0         NaN     b''  BENIGN  \n",
      "3  16384      8235.0             5.0         NaN     b''  BENIGN  \n",
      "4  16384      8235.0             5.0         NaN     b''  BENIGN  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_benign = pd.read_csv(os.path.join(\"train_set\", \"benign3.csv\"))\n",
    "df_benign[\"label\"] = \"BENIGN\"\n",
    "df_attack = pd.read_csv(os.path.join(\"dataset_lite\", \"ssldosA10only.csv\"))\n",
    "df_attack[\"label\"] = \"ATTACK\"\n",
    "df_mix = pd.concat([df_benign, df_attack], axis=0)\n",
    "print(df_mix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_mix.groupby([\"src_ip\", \"src_port\", \"dst_ip\", \"dst_port\", \"protocol\"])\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "df_mix[\"group\"] = df_group.ngroup()\n",
    "df_mix[\"group\"] = df_mix[\"group\"] % 10\n",
    "df_mix[\"group\"] = df_mix[\"group\"].apply(lambda x: \"train\" if x < 8 else \"test\")\n",
    "\n",
    "df_train = df_mix[df_mix[\"group\"] == \"train\"]\n",
    "df_test = df_mix[df_mix[\"group\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import Packet, Flow\n",
    "\n",
    "def get_flows(df: pd.DataFrame, key_type: str = \"default\") -> dict:\n",
    "    mp = dict()\n",
    "    for idx in range(len(df)): # simulate the process of packet processing\n",
    "        row = df.iloc[idx]\n",
    "        pkt = Packet(\n",
    "            src_addr=row[\"src_addr\"],\n",
    "            dst_addr=row[\"dst_addr\"],\n",
    "            src_ip=row[\"src_ip\"],\n",
    "            dst_ip=row[\"dst_ip\"],\n",
    "            src_port=row[\"src_port\"],\n",
    "            dst_port=row[\"dst_port\"],\n",
    "            protocol=row[\"protocol\"],\n",
    "            proto_code=row[\"proto_code\"],\n",
    "            pkt_length=row[\"pkt_length\"],\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            ttl=row[\"ttl\"],\n",
    "            tcp_window=row[\"tcp_window\"],\n",
    "            tcp_dataoffset=row[\"tcp_dataoffset\"],\n",
    "            udp_length=row[\"udp_length\"],\n",
    "            label=row[\"label\"],\n",
    "        )\n",
    "        key = pkt.key(type=key_type)\n",
    "        if key not in mp:\n",
    "            mp[key] = Flow()\n",
    "        mp[key].add_packet(pkt)\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import whisper_config\n",
    "\n",
    "def transform(mp: dict, feature_type: str = \"whisper\", data_type: str = \"train\"):\n",
    "    packet_data = []\n",
    "    for key, flow in mp.items():\n",
    "        vec = flow.vector(feature_type=feature_type)\n",
    "        if feature_type == \"whisper\":\n",
    "            if len(vec) <= (whisper_config[\"n_fft\"] // 2):\n",
    "                # packet level features\n",
    "                data = flow.packet_vector_simple()\n",
    "                packet_data += data\n",
    "            else: # flow level features\n",
    "                pass\n",
    "        else: # for other feature types\n",
    "            pass\n",
    "    return packet_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import json\n",
    "import skops.io as sio\n",
    "\n",
    "def train(data, labels, save_path):\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    clf.fit(data, labels)\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    sio.dump(clf, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow_dict = get_flows(df_train)\n",
    "train_packet_data = transform(train_flow_dict, feature_type=\"whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "save_path = os.path.join(\"model\", \"dt\", \"model.skops\")\n",
    "train_data = np.array(train_packet_data)[:,:-1]\n",
    "train_labels = np.array(train_packet_data)[:,-1]\n",
    "train(data, labels, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flow_dict = get_flows(df_test)\n",
    "test_packet_data = transform(test_flow_dict, feature_type=\"whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sio.load(save_path, True)\n",
    "test_data = np.array(test_packet_data)[:,:-1]\n",
    "test_labels = np.array(test_packet_data)[:,-1]\n",
    "\n",
    "pred = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994699178372648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(data, labels, load_path):\n",
    "    clf = sio.load(load_path, True)\n",
    "    pred = clf.predict(data)\n",
    "    return accuracy_score(labels, pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test short flows with more datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "benign_filenames = [os.path.join(\"train_set\", \"benign\" + str(i) + \".csv\") for i in range(4, 6)]\n",
    "attack_filenames = [os.path.join(\"dataset_lite\", x) for x in os.listdir(\"dataset_lite\") if x.endswith(\".csv\")] + [\"mirai.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign train_set/benign4.csv accuracy: 0.9993577017400443\n",
      "benign train_set/benign5.csv accuracy: 0.9995105836290223\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import skops.io as sio\n",
    "\n",
    "load_path = os.path.join(\"model\", \"dt\", \"model.skops\")\n",
    "\n",
    "for filename in benign_filenames:\n",
    "    test_df = pd.read_csv(filename)\n",
    "    test_df[\"label\"] = \"BENIGN\"\n",
    "    test_flow_dict = get_flows(test_df)\n",
    "    test_packet_data = transform(test_flow_dict, feature_type=\"whisper\")\n",
    "    test_data = np.array(test_packet_data)[:,:-1]\n",
    "    test_labels = np.array(test_packet_data)[:,-1]\n",
    "    acc = test(test_data, test_labels, load_path)\n",
    "    print(f\"benign {filename} accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack dataset_lite/osscan.csv accuracy: 0.0928540864356012\n",
      "attack dataset_lite/ssldosA10only.csv accuracy: 1.0\n",
      "attack dataset_lite/BruteForce-Web.csv accuracy: 0.39919354838709675\n",
      "attack dataset_lite/SQL_Injection.csv accuracy: 0.38202247191011235\n",
      "attack dataset_lite/BruteForce-XSS.csv accuracy: 0.56\n",
      "attack mirai.csv accuracy: 0.8087250657720737\n"
     ]
    }
   ],
   "source": [
    "for filename in attack_filenames:\n",
    "    test_df = pd.read_csv(filename)\n",
    "    test_df[\"label\"] = \"ATTACK\"\n",
    "    test_flow_dict = get_flows(test_df)\n",
    "    test_packet_data = np.array(transform(test_flow_dict, feature_type=\"whisper\"))\n",
    "    if test_packet_data.shape[0] == 0:\n",
    "        continue\n",
    "    test_data = test_packet_data[:,:-1]\n",
    "    test_labels = test_packet_data[:,-1]\n",
    "    acc = test(test_data, test_labels, load_path)\n",
    "    print(f\"attack {filename} accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c4067ac78a363bc0422166851ee04b79641d0bfa5e795e52c96a7ffb7a6500a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
